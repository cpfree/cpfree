# design-question

## 为什么zookeeper节点是奇数

在每台机器数据保持一致的情况下，zookeeper集群可以保证，客户端发起的每次查询操作，集群节点都能返回同样的结果。

但是对于客户端发起的修改、删除等能改变数据的操作呢？集群中那么多台机器，你修改你的，我修改我的，最后返回集群中哪台机器的数据呢？

这就是一盘散沙，需要一个领导，于是在zookeeper集群中，leader的作用就体现出来了，只有leader节点才有权利发起修改数据的操作，而follower节点即使接收到了客户端发起的修改操作，也要将其转交给leader来处理，leader接收到修改数据的请求后，会向所有follower广播一条消息，让他们执行某项操作，follower 执行完后，便会向 leader 回复执行完毕。当 leader 收到半数以上的 follower 的确认消息，便会判定该操作执行完毕，然后向所有 follower 广播该操作已经生效。

所以zookeeper集群中leader是不可缺少的，但是 leader 节点是怎么产生的呢？其实就是由所有follower 节点选举产生的，讲究民主嘛，而且leader节点只能有一个，毕竟一个国家不能有多个总统。

这个时候回到我们的小标题，为什么 zookeeper 节点数是奇数，我们下面来一一来说明：

1. 容错率

   首先从容错率来说明：（需要保证集群能够有半数进行投票）

   2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉，但是相对于 单节点服务器，2台服务器还有两个单点故障，所以直接排除了。

   3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉

   4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉

   5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉

2. 防脑裂

   脑裂集群的脑裂通常是发生在节点之间通信不可达的情况下，集群会分裂成不同的小集群，小集群各自选出自己的leader节点，导致原有的集群出现多个leader节点的情况，这就是脑裂。

   3台服务器，投票选举半数为1.5，一台服务裂开，和另外两台服务器无法通行，这时候2台服务器的集群（2票大于半数1.5票），所以可以选举出leader，而 1 台服务器的集群无法选举。

   4台服务器，投票选举半数为2，可以分成 1,3两个集群或者2,2两个集群，对于 1,3集群，3集群可以选举；对于2,2集群，则不能选择，造成没有leader节点。

   5台服务器，投票选举半数为2.5，可以分成1,4两个集群，或者2,3两集群，这两个集群分别都只能选举一个集群，满足zookeeper集群搭建数目。

   以上分析，我们从容错率以及防止脑裂两方面说明了3台服务器是搭建集群的最少数目，4台发生脑裂时会造成没有leader节点的错误。

## zookeeper 读写

写处理它比在 ZooKeeper 集群过程是昂贵的

action | summary
-|-
写入 | 写过程是由领导节点处理。领导者转发写请求到所有znodes及其等待来自znodes应答。如果一半的znodes的回复，那么写入过程就完成了。
读取 | 读取在内部由特定连接znode进行的，所以没有必要与集群交互。  
复制数据库 | 它是用来将数据存储在zookeeper。每个znode都有自己的数据库及其每个znode 在一致性的作用下，每次有相同的数据。  
领导者（节点） | 领导者是由Znode负责处理写请求。  
追随者（节点） | 追随者收到来自客户端的写请求，并将其转发到领导znode。  
请求处理器 | 目前仅在领导节点。它从跟随节点的请求支配写入。  
原子广播 | 负责从领导节点到从节点广播更改。

## ZooKeeper分布式锁(排他锁)

分布式锁有多种实现方式，比如通过数据库、redis都可实现。作为分布式协同工具ZooKeeper，当然也有着标准的实现方式。本文介绍在zookeeper中如何实现排他锁。

1.0版本
首先我们先介绍一个简单的zookeeper实现分布式锁的思路：

用zookeeper中一个临时节点代表锁，比如在/exlusive_lock下创建临时子节点/exlusive_lock/lock。
所有客户端争相创建此节点，但只有一个客户端创建成功。
创建成功代表获取锁成功，此客户端执行业务逻辑
未创建成功的客户端，监听/exlusive_lock变更
获取锁的客户端执行完成后，删除/exlusive_lock/lock，表示锁被释放
锁被释放后，其他监听/exlusive_lock变更的客户端得到通知，再次争相创建临时子节点/exlusive_lock/lock。此时相当于回到了第2步。
我们的程序按照上述逻辑直至抢占到锁，执行完业务逻辑。

上述是较为简单的分布式锁实现方式。能够应付一般使用场景，但存在着如下两个问题：

1、锁的获取顺序和最初客户端争抢顺序不一致，这不是一个公平锁。每次锁获取都是当次最先抢到锁的客户端。

2、羊群效应，所有没有抢到锁的客户端都会监听/exlusive_lock变更。当并发客户端很多的情况下，所有的客户端都会接到通知去争抢锁，此时就出现了羊群效应。

为了解决上面的问题，我们重新设计。

2.0版本
我们在2.0版本中，让每个客户端在/exlusive_lock下创建的临时节点为有序节点，这样每个客户端都在/exlusive_lock下有自己对应的锁节点，而序号排在最前面的节点，代表对应的客户端获取锁成功。排在后面的客户端监听自己前面一个节点，那么在他前序客户端执行完成后，他将得到通知，获得锁成功。逻辑修改如下：

每个客户端往/exlusive_lock下创建有序临时节点/exlusive_lock/lock_。创建成功后/exlusive_lock下面会有每个客户端对应的节点，如/exlusive_lock/lock_000000001
客户端取得/exlusive_lock下子节点，并进行排序，判断排在最前面的是否为自己。
如果自己的锁节点在第一位，代表获取锁成功，此客户端执行业务逻辑
如果自己的锁节点不在第一位，则监听自己前一位的锁节点。例如，自己锁节点lock_000000002，那么则监听lock_000000001.
当前一位锁节点（lock_000000001）对应的客户端执行完成，释放了锁，将会触发监听客户端（lock_000000002）的逻辑。
监听客户端重新执行第2步逻辑，判断自己是否获得了锁。
如此修改后，每个客户端只关心自己前序锁是否释放，所以每次只会有一个客户端得到通知。而且，所有客户端的执行顺序和最初锁创建的顺序是一致的。解决了1.0版本的两个问题。

————————————————
(ZooKeeper分布式锁实现java例子，附完整可运行源代码)
版权声明：本文为CSDN博主「稀有气体」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：(https://blog.csdn.net/liyiming2017/article/details/83786331)
